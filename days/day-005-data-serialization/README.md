# Day 5: Data Serialization Comparison

## üìñ Learning Objectives (15 min)

**Time**: 1 hour


By the end of today, you will:
- Compare all data formats (CSV, JSON, Parquet, Avro, Arrow)
- Understand when to use each format
- Benchmark performance across formats
- Make informed format decisions
- Build a format decision matrix

---

## Theory

### What is Data Serialization?

**Serialization**: Converting data structures into a format that can be stored or transmitted.

**Deserialization**: Converting stored/transmitted data back into data structures.

### The 5 Formats We've Learned

| Format | Type | Schema | Use Case |
|--------|------|--------|----------|
| **CSV** | Row, Text | No | Simple tabular data |
| **JSON** | Row, Text | No | APIs, configs |
| **Parquet** | Columnar, Binary | Optional | Analytics, data lakes |
| **Avro** | Row, Binary | Required | Streaming, Kafka |
| **Arrow** | Columnar, Memory | Yes | In-memory processing |

### Detailed Comparison

#### 1. Storage Type

**Row-Based** (CSV, JSON, Avro):
- Stores complete records together
- Fast for reading entire rows
- Good for transactional workloads

**Columnar** (Parquet, Arrow):
- Stores columns together
- Fast for reading specific columns
- Good for analytical workloads

#### 2. Format Type

**Text-Based** (CSV, JSON):
- Human-readable
- Larger file size
- Slower to parse
- No type information

**Binary** (Parquet, Avro, Arrow):
- Not human-readable
- Smaller file size
- Faster to parse
- Type information included

#### 3. Schema Support

**No Schema** (CSV, JSON):
- Flexible but error-prone
- No validation
- Types inferred at read time

**Optional Schema** (Parquet):
- Schema stored in file
- Self-describing
- Types preserved

**Required Schema** (Avro, Arrow):
- Schema must be defined
- Strong validation
- Schema evolution support

### Performance Comparison

```python
import pandas as pd
import time
import os

# Create test data
df = pd.DataFrame({
    'id': range(100000),
    'name': [f'User_{i}' for i in range(100000)],
    'value': range(100000),
    'category': ['A', 'B', 'C', 'D'] * 25000
})

# Write and measure
formats = {}

# CSV
start = time.time()
df.to_csv('test.csv', index=False)
formats['CSV'] = {
    'write_time': time.time() - start,
    'size': os.path.getsize('test.csv')
}

# JSON
start = time.time()
df.to_json('test.json', orient='records')
formats['JSON'] = {
    'write_time': time.time() - start,
    'size': os.path.getsize('test.json')
}

# Parquet
start = time.time()
df.to_parquet('test.parquet', compression='snappy')
formats['Parquet'] = {
    'write_time': time.time() - start,
    'size': os.path.getsize('test.parquet')
}

# Print results
for fmt, stats in formats.items():
    print(f"{fmt}:")
    print(f"  Write: {stats['write_time']:.2f}s")
    print(f"  Size: {stats['size'] / 1024 / 1024:.2f} MB")
```

**Typical Results** (100K rows):
```
CSV:
  Write: 0.5s
  Size: 5.2 MB

JSON:
  Write: 1.2s
  Size: 8.5 MB

Parquet:
  Write: 0.3s
  Size: 1.8 MB
```

### Decision Matrix

#### Use CSV When:
‚úÖ Data is simple and tabular
‚úÖ Need human-readable format
‚úÖ Working with spreadsheets (Excel)
‚úÖ Small datasets (<100MB)
‚úÖ Maximum compatibility needed

‚ùå Don't use for:
- Large datasets
- Complex nested data
- Need type preservation
- Performance critical

#### Use JSON When:
‚úÖ REST APIs
‚úÖ Configuration files
‚úÖ Nested/hierarchical data
‚úÖ Web applications
‚úÖ Human-readable needed

‚ùå Don't use for:
- Large datasets
- High performance needed
- Storage efficiency critical
- Analytics workloads

#### Use Parquet When:
‚úÖ Data warehouse/lake
‚úÖ Analytics queries
‚úÖ Large datasets (>1GB)
‚úÖ Read-heavy workloads
‚úÖ Need compression
‚úÖ Columnar access patterns

‚ùå Don't use for:
- Streaming data
- Frequent schema changes
- Need human-readable
- Small datasets

#### Use Avro When:
‚úÖ Kafka streaming
‚úÖ Schema evolution needed
‚úÖ Microservices communication
‚úÖ Write-heavy workloads
‚úÖ RPC/messaging systems

‚ùå Don't use for:
- Analytics queries
- Human-readable needed
- Simple data exchange
- No schema changes

#### Use Arrow When:
‚úÖ In-memory processing
‚úÖ Data exchange between systems
‚úÖ Zero-copy operations needed
‚úÖ Language interoperability
‚úÖ Fast analytics in RAM

‚ùå Don't use for:
- Long-term storage
- Simple data exchange
- Small datasets

### Real-World Scenarios

#### Scenario 1: E-commerce Analytics
**Data**: Order history, 10M rows, read-heavy

**Best Choice**: Parquet
- Columnar for analytics
- Excellent compression
- Fast column reads
- Partitioning support

#### Scenario 2: Real-Time Events
**Data**: User clicks, streaming, schema changes

**Best Choice**: Avro
- Kafka standard
- Schema evolution
- Compact binary
- Fast serialization

#### Scenario 3: REST API
**Data**: User profiles, nested data, web clients

**Best Choice**: JSON
- Human-readable
- Native web support
- Nested structures
- Easy debugging

#### Scenario 4: Data Export
**Data**: Database table, 1000 rows, for Excel

**Best Choice**: CSV
- Excel compatible
- Simple format
- Human-readable
- Universal support

#### Scenario 5: ML Pipeline
**Data**: Features, in-memory, Python to Spark

**Best Choice**: Arrow
- Zero-copy transfer
- Fast in-memory
- Language agnostic
- Efficient processing

### Conversion Between Formats

```python
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

# Read CSV
df = pd.read_csv('data.csv')

# Convert to Parquet
df.to_parquet('data.parquet', compression='snappy')

# Convert to JSON
df.to_json('data.json', orient='records')

# Convert to Arrow
table = pa.Table.from_pandas(df)

# Arrow to Parquet
pq.write_table(table, 'data_from_arrow.parquet')

# Parquet to Arrow
table = pq.read_table('data.parquet')

# Arrow to Pandas
df = table.to_pandas()
```

### Compression Comparison

```python
import pandas as pd

df = pd.DataFrame({'col': range(1000000)})

# Parquet with different compressions
df.to_parquet('snappy.parquet', compression='snappy')
df.to_parquet('gzip.parquet', compression='gzip')
df.to_parquet('zstd.parquet', compression='zstd')
df.to_parquet('none.parquet', compression=None)

# Compare sizes
import os
for file in ['snappy', 'gzip', 'zstd', 'none']:
    size = os.path.getsize(f'{file}.parquet')
    print(f"{file}: {size / 1024:.2f} KB")
```

### Quick Reference Guide

```
Need human-readable? ‚Üí CSV or JSON
Need smallest size? ‚Üí Parquet (compressed)
Need fastest read? ‚Üí Parquet (columnar) or Arrow (in-memory)
Need schema evolution? ‚Üí Avro
Need streaming? ‚Üí Avro (Kafka)
Need analytics? ‚Üí Parquet
Need API? ‚Üí JSON
Need Excel? ‚Üí CSV
Need in-memory speed? ‚Üí Arrow
```

---

## üíª Exercises (40 min)

Open `exercise.py` and complete the tasks.

### Exercise 1: Format Benchmark
- Create dataset with 100K rows
- Write to all 5 formats
- Measure write times
- Compare file sizes
- Create comparison chart

### Exercise 2: Read Performance
- Read each format
- Measure read times
- Read specific columns (where applicable)
- Compare performance

### Exercise 3: Format Conversion
- Read CSV file
- Convert to Parquet, JSON, Avro
- Verify data integrity
- Compare sizes

### Exercise 4: Decision Matrix
- Given 5 scenarios
- Choose best format for each
- Justify your choice

### Exercise 5: Compression Test
- Write Parquet with different compressions
- Compare sizes and speeds
- Determine best compression

### Exercise 6: Real-World Pipeline
- Simulate data pipeline
- CSV ‚Üí Parquet ‚Üí Arrow ‚Üí Processing
- Measure end-to-end performance

---

## ‚úÖ Quiz (5 min)

Answer these questions in `quiz.md`:

1. Which format is smallest: CSV, JSON, or Parquet?
2. Which format is best for Kafka streaming?
3. Which format is human-readable?
4. Which format is best for analytics?
5. Which format supports schema evolution?
6. Which format is columnar?
7. When would you use CSV over Parquet?
8. What is Arrow best used for?

---

## üéØ Key Takeaways

- **No single format is best** - choose based on use case
- **CSV/JSON** - simple, human-readable, but inefficient
- **Parquet** - best for analytics and data lakes
- **Avro** - best for streaming and Kafka
- **Arrow** - best for in-memory processing
- **Consider**: size, speed, schema, use case
- **Parquet is 70-80% smaller** than CSV
- **Format conversion is easy** with Pandas/Arrow

---

## üìö Additional Resources

- [Data Format Comparison](https://www.databricks.com/glossary/data-serialization)
- [Choosing the Right Format](https://towardsdatascience.com/csv-json-parquet-avro-choosing-the-right-format)
- [Parquet vs Avro](https://www.upsolver.com/blog/apache-parquet-vs-apache-avro)

---

## Tomorrow: Day 6 - Compression Algorithms

We'll dive deep into compression algorithms (Snappy, ZSTD, LZ4, Gzip) and learn when to use each.
